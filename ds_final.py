# -*- coding: utf-8 -*-
"""ds project(eye cataract).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UQK5py502XVU4fv4DD7Xq6ZNaPDIisOK

# **EYE CATARACT CLASSIFICATION**

**IMPORTING DRIVE**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**IMPORTING LIBRARIES**"""

import numpy as np    #numerical data and dataframes
import pandas as pd
import os             #navigate through directories
import matplotlib.pyplot as plt   #visualization
import seaborn as sns
import cv2                       # reads,process images
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler # en cat lab nd sc num data
from sklearn.decomposition import PCA  #reduces image dim
from sklearn.svm import SVC  # svm for classification
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV

"""**PATH OF THE DATASET**"""

train_dir = "/content/drive/MyDrive/DS CATARACT /train"

"""**PREPROCESSING AND DISPLAYING IMAGES**"""

# Preprocess
def load_images(directory, img_size=(128, 128)):
    images, labels, original_images = [], [], []
    class_labels = os.listdir(directory)
    for label in class_labels:
        class_dir = os.path.join(directory, label)
        for img_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_name)
            img = cv2.imread(img_path)
            if img is not None:
                original_images.append(img)
                img = cv2.resize(img, img_size)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                img = cv2.GaussianBlur(img, (5, 5), 0)
                images.append(img.flatten())
                labels.append(label)

    # Display images
    fig, axes = plt.subplots(1, 10, figsize=(20, 5))
    for i in range(10):
        axes[i].imshow(original_images[i], cmap='gray')
        axes[i].set_title(f"Label: {labels[i]}") # Changed y[i] to labels[i]
        axes[i].axis('off')
    plt.tight_layout()
    plt.show()

    return np.array(images), np.array(labels), original_images
X, y, original_images = load_images(train_dir)

"""**ENCODE LABELS**"""

le = LabelEncoder()
y = le.fit_transform(y)

print("Classes:", le.classes_)  # shows what 0 and 1 mean

"""**NORMALIZE FEATURES**"""

# STEP 1: Train-Test Split FIRST
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

# STEP 2: Scale ONLY using training data
# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# PCA (Keep 95% variance automatically)
pca = PCA(n_components=0.95)

X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("Original number of features:", X_train.shape[1])
print("Reduced number of features after PCA:", X_train_pca.shape[1])

from sklearn.model_selection import cross_val_score

# Temporary model for cross validation
svm_temp = SVC(kernel='rbf', C=10, gamma=0.01)

scores = cross_val_score(svm_temp, X_train_pca, y_train, cv=5)

print("Cross Validation Scores:", scores)
print("Mean CV Accuracy:", scores.mean() * 100)

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1],
    'kernel': ['rbf']
}

grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train_pca, y_train)

print("Best Parameters:", grid.best_params_)
print("Best Cross Validation Score:", grid.best_score_ * 100)

# Use best model
svm_model = grid.best_estimator_

"""**TRAIN SVM**"""

svm_model = grid.best_estimator_

"""**MAKE PREDICTIONS**"""

y_pred = svm_model.predict(X_test_pca)

"""**EVALUATING MODEL**"""

accuracy = accuracy_score(y_test, y_pred)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""**TRAIN SVM CLASSIFIER**"""

svm = SVC(kernel='linear', random_state=0)

# Train using PCA data
svm.fit(X_train_pca, y_train)

"""**PREDICT AND EVALUATE**"""

# Train
svm_linear = SVC(kernel='linear', random_state=0)
svm_linear.fit(X_train_pca, y_train)

# Predict on both
y_pred_val  = svm_linear.predict(X_val_pca)
y_pred_test = svm_linear.predict(X_test_pca)

# Accuracy on both
print(f"Linear SVM Validation Accuracy : {accuracy_score(y_val, y_pred_val) * 100:.2f}%")
print(f"Linear SVM Test Accuracy       : {accuracy_score(y_test, y_pred_test) * 100:.2f}%")

# Confusion matrix on TEST
cm = confusion_matrix(y_test, y_pred_test)
print("\nConfusion Matrix:")
print(cm)

# Classification report on TEST
print("\nClassification Report:")
print(classification_report(y_test, y_pred_test, target_names=le.classes_))

"""**CONFUSION MATRIX**"""

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)

"""**PREDICTING FOR THE SINGLE IMAGE**"""

def classify_new_image(image_path):
    from google.colab.patches import cv2_imshow
    import matplotlib.pyplot as plt
    img = cv2.imread(image_path)
    if img is not None:
        img = cv2.resize(img, (128, 128))
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        img_flattened = blurred.flatten().reshape(1, -1)
        img_scaled = scaler.transform(img_flattened)
        img_scaled = img_scaled.reshape(1, -1)
        img_pca = pca.transform(img_scaled)
        prediction = svm_model.predict(img_pca)
        label_name = le.inverse_transform(prediction)[0]  # ← ADD HERE
        print(f"Prediction: {label_name}")                # ← REPLACE old print
        plt.imshow(img, cmap='gray')
        plt.title(f"Prediction: {label_name}")            # ← REPLACE old title
        plt.axis('off')
        plt.show()
    else:
        print("Image not found or unable to read")

# ↓ REPLACE your old hardcoded path with these 4 lines
sample_class = os.listdir(train_dir)[0]
sample_img   = os.listdir(os.path.join(train_dir, sample_class))[0]
sample_path  = os.path.join(train_dir, sample_class, sample_img)
classify_new_image(sample_path)

new_image_path = "/content/drive/MyDrive/DS CATARACT /train/mature/10_JPG.rf.20ec8d62d6f05afac7ac62a95ac10264.jpg"
classify_new_image(new_image_path)

"""## **CLUSTERING**

**LOADING LIBRARIES**
"""

import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from collections import Counter

"""**PATH OF THE DATASET**"""

train_dir = '/content/drive/MyDrive/DS CATARACT /train'

"""**PREPROCESSING IMAGES**"""

def load_images(directory, img_size=(128, 128)):
    images, labels, original_images, processed_images = [], [], [], []
    if not os.path.exists(directory):
        raise FileNotFoundError(f"Dataset directory not found: {directory}")

    class_labels = os.listdir(directory)
    for label in class_labels:
        class_dir = os.path.join(directory, label)
        for img_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_name)
            img = cv2.imread(img_path)
            if img is not None:
                original_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Store original image
                img = cv2.resize(img, img_size)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
                img = cv2.GaussianBlur(img, (5,5), 0)  # Apply Gaussian blur
                processed_images.append(img)  # Store processed image
                images.append(img.flatten())  # Flatten image for clustering
                labels.append(label)
    return np.array(images), np.array(labels), original_images, processed_images

X, y, original_images, processed_images = load_images(train_dir)

"""**DIPLAYING ORIGINAL AND PREPROCESSED IMAGES**"""

fig, axes = plt.subplots(2, 10, figsize=(20, 5))
for i in range(10):
    axes[0, i].imshow(original_images[i])
    axes[0, i].set_title(f"Original {i}")
    axes[0, i].axis("off")

    axes[1, i].imshow(processed_images[i], cmap='gray')
    axes[1, i].set_title(f"Processed {i}")
    axes[1, i].axis("off")
plt.show()

"""**NORMALIZE FEATURES**"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
for k in range(1, 10):
    km = KMeans(n_clusters=k, random_state=0, n_init=10)  # add n_init=10 # Added n_init to avoid warning
    km.fit(X_train_pca) # Corrected to fit the current km object
    inertia.append(km.inertia_)

plt.plot(range(1,10), inertia)
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

"""**K-MEANS CLUSTERING**"""

num_clusters = 3

kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
kmeans.fit(X_train_pca)

labels = kmeans.labels_

"""**COUNTING NUMBER OF SAMPLES IN EACH CLUSTER**"""

cluster_counts = Counter(labels)
print("Cluster Distribution:", cluster_counts)

"""**CLUSTER DISTRIBUTION**"""

plt.figure(figsize=(6, 6))
plt.pie(cluster_counts.values(), labels=[f'Cluster {i}' for i in cluster_counts.keys()], autopct='%1.1f%%', colors=['red', 'yellow', 'green'])
plt.title("Cluster Distribution")
plt.show()

"""**DISPLAYING SAMPLE IMAGES FROM EACH CLUSTER**"""

def display_cluster_images(X, labels, original_images, cluster_id, num_images=9):
    cluster_indices = np.where(labels == cluster_id)[0][:num_images]
    plt.figure(figsize=(10, 10))
    for i, idx in enumerate(cluster_indices):
        plt.subplot(3, 3, i + 1)
        plt.imshow(original_images[idx])
        plt.title(f"Cluster {cluster_id}")
        plt.axis("off")
    plt.show()

for cluster_id in range(num_clusters):
    print(f"Displaying images for Cluster {cluster_id}:")
    display_cluster_images(X, labels, original_images, cluster_id)

"""**SCATTER PLOT**"""

# Replace entire scatter plot with:
plt.figure(figsize=(8, 6))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],
            c=labels, cmap='viridis', alpha=0.6)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("KMeans Clusters (PCA Space)")
plt.colorbar(label="Cluster")
plt.show()

"""**BARPLOT**"""

sns.barplot(x=list(cluster_counts.keys()),
            y=list(cluster_counts.values()),
            hue=list(cluster_counts.keys()),  # ← add this
            palette=['blue','orange','green'],
            legend=False)                      # ← add this
